# NLP-Enriched News Intelligence Platform

## Project Overview
The goal of this project is to develop a platform that integrates Natural Language Processing (NLP) techniques to provide insightful analysis of news articles. The platform connects to a news data source, processes the articles through various NLP tasks, and outputs enriched data. It performs the following core tasks:

1. **Entity Detection** - Identifies and categorizes organizations (companies) mentioned in the articles.
2. **Topic Detection** - Classifies news articles into predefined topics such as Tech, Sport, Business, Entertainment, or Politics.
3. **Sentiment Analysis** - Assesses the sentiment (positive, negative, or neutral) of each article.
4. **Scandal Detection** - Detects potential environmental scandals associated with the companies mentioned in the articles.



## Setup Instructions

### Prerequisites
- Python 3.10
- Install required dependencies by running:
  
  ```bash
  conda create --name env --file requirements.txt
  ```

### Scraper Setup

The **scraper_news.py** script is responsible for fetching news articles from a designated source and saving them into a  SQL database. The scraper should collect at least 300 articles over the past week for analysis.

Run the scraper:

```bash
python3 scraper_news.py
```

### NLP Engine Setup

The **nlp_enriched_news.py** script processes the collected news articles by applying various NLP techniques. These include entity detection, topic classification, sentiment analysis, and scandal detection. It outputs a CSV file (`enhanced_news.csv`) with enriched information for each article.

Run the NLP processing:

```bash
python3 nlp_enriched_news.py
```

### Output

After running the NLP engine, you will get the enriched data stored in `enhanced_news.csv`. The file will include the following columns:

- **Unique ID**: Unique identifier for the article.
- **URL**: URL of the article.
- **Date**: Date when the article was scraped.
- **Headline**: Headline of the article.
- **Body**: Body of the article.
- **Org**: List of companies/organizations detected.
- **Topics**: List of topics classified (Tech, Sport, Business, Entertainment, Politics).
- **Sentiment**: Sentiment score (float).
- **Scandal_distance**: Metric indicating the likelihood of an environmental scandal.
- **Top_10**: Boolean flag marking the top 10 articles based on scandal detection.



## Core Features

### 1. **Entity Detection**

This task extracts organizations (ORG entities) mentioned in the article's body and headline. We use **SpaCy** for Named Entity Recognition (NER) to identify companies and organizations.

### 2. **Topic Detection**

The classifier categorizes articles into topics using a pre-trained model based on a labeled dataset. The available topics include:
- Tech
- Sport
- Business
- Entertainment
- Politics

The classifier is trained using `scikit-learn`, and its learning curves are saved to `learning_curves.png`.

### 3. **Sentiment Analysis**

Sentiment analysis classifies each article as positive, negative, or neutral. We use a **pre-trained NLTK model** for sentiment analysis to quickly analyze the sentiment without needing to train a custom model.

### 4. **Scandal Detection**

Voici une version mise à jour de la méthodologie, expliquant clairement l'intégration de **tous les mots-clés** dans le processus d'unification du score, tout en précisant l'importance de l'**Article ID** :

## Methodology

### 4.1. Define Keywords

We define a list of environmental disaster keywords that may be caused by companies, such as:

- **Pollution**
- **Deforestation**
- **Climate Change**
- **Oil Spill**
- **Desertification**

These keywords are selected to focus on specific environmental issues caused by companies and avoid ambiguity (e.g., "flood" could refer to both natural and man-made disasters).

### 4.2. Compute Keyword Embeddings

We compute **embeddings** for each keyword using NLP models like spaCy or transformers. Embeddings are high-dimensional vector representations of words that capture their meaning in context, which helps in comparing them against sentences in the article.

We use word embeddings generated by spaCy because they provide a rich semantic representation that allows us to compare the meaning of words, even in different contexts. These embeddings take into account the context of the word within a sentence, making them more accurate than traditional methods like bag-of-words.

### 4.3. Calculate Similarity Between Keywords and Sentences

For each sentence containing an organization (e.g., a company name), we calculate the **cosine similarity** between the embeddings of the keyword and the sentence. Cosine similarity measures the semantic closeness between two pieces of text, with values ranging from -1 (completely dissimilar) to 1 (highly similar).

We use cosine similarity because it is a well-established method in NLP for comparing text similarity. Cosine similarity is ideal for this task as it is efficient and effective at identifying the degree of similarity between keywords and the content of the article.



### 4.4. Unify Scores Across Sentences and Articles

For each article, we:

- Calculate the similarity between each sentence containing an organization and each keyword.
- For each keyword, the **maximum similarity score** across all sentences in the article is selected. This reflects the most relevant sentence for each keyword.
- Finally, compute the **average** of these maximum similarity scores for all keywords to get a **final score** for the article.

The approach of aggregating the maximum similarity per keyword and then computing the average ensures that we focus on the most relevant parts of the article, capturing the overall relevance to the environmental disaster theme

### Example: Unifying Scores for an Article (Article ID = 12345)

Consider the following article with sentences mentioning organizations (e.g., **PetroCorp**). We calculate the similarity for **all keywords** defined previously.

| **Article ID** | **Sentence**                                                                                             | **Oil Spill** | **Pollution** | **Deforestation** | **Climate Change** | **Desertification** |
|----------------|----------------------------------------------------------------------------------------------------------|---------------|---------------|-------------------|--------------------|---------------------|
| 12345          | "PetroCorp caused an oil spill in the Mediterranean."                                                     | 0.85          | 0.45          | 0.10              | 0.30               | 0.20                |
| 12345          | "Environmental groups criticized the pollution caused by PetroCorp."                                      | 0.30          | 0.90          | 0.15              | 0.40               | 0.25                |
| 12345          | "PetroCorp’s actions are linked to deforestation in the Amazon."                                          | 0.20          | 0.40          | 0.80              | 0.50               | 0.35                |
| 12345          | "PetroCorp is contributing to climate change through emissions."                                          | 0.15          | 0.60          | 0.10              | 0.85               | 0.40                |
| 12345          | "Experts warn that PetroCorp's practices could lead to desertification of local land."                   | 0.25          | 0.50          | 0.20              | 0.60               | 0.90                |

---

### Step 1: Maximum Similarity per Keyword

For each keyword, we take the **maximum similarity score** across all sentences:

| **Keyword**      | **Max Similarity (Article ID: 12345)** |
|------------------|----------------------------------------|
| **Oil Spill**    | 0.85                                   |
| **Pollution**    | 0.90                                   |
| **Deforestation**| 0.80                                   |
| **Climate Change**| 0.85                                  |
| **Desertification**| 0.90                                |

---

### Step 2: Calculate the Final Score for the Article

The **final score** for the article is computed by averaging the **maximum similarity scores** for each keyword.

**Final Score** = (0.85 + 0.90 + 0.80 + 0.85 + 0.90) / 5 = **0.86**

This score indicates the degree of relevance to environmental disasters caused by the company **PetroCorp**.

---



### 5. **Source Analysis** (Optional)

This optional feature generates insights into the news sources, such as:
- The proportion of topics per day.
- Sentiment per company.
- The most mentioned companies.

These insights are plotted and saved in the `results/insights` folder.


## How to Run the Project

1. **Run the Scraper**: Collect at least 300 articles by running `scraper_news.py`.

   ```bash
   python3 scraper_news.py
   ```

2. **Process the Articles**: Use `nlp_enriched_news.py` to process the scraped articles.

   ```bash
   python3 nlp_enriched_news.py
   ```

3. **Review the Results**: Check the `results/insights` folder for the CSV file containing the enriched data and the saved model files.

---

Here’s how you can write the resources section for your README file using the links you provided:

---

## Resources

Here are some useful resources for setting up and learning more about different tools and techniques used in this project:

1. **Anaconda Installation**  
   Download the latest Anaconda installer for Linux to set up your Python environment.  
   Command to download Anaconda:
   ```bash
   curl -O https://repo.anaconda.com/archive/Anaconda3-2024.10-1-Linux-x86_64.sh
   ```

2. **Text Preprocessing with spaCy**  
   A comprehensive guide to text preprocessing in Natural Language Processing (NLP) using spaCy.  
   [Text Preprocessing with spaCy](https://medium.com/eni-digitalks/text-preprocessing-nlp-fundamentals-with-spacy-54f32e520bc8)

3. **Learning Curves with scikit-learn**  
   Explore how to plot learning curves in machine learning using scikit-learn.  
   [Learning Curves Example](https://scikit-learn.org/dev/auto_examples/model_selection/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py)

4. **Saving Machine Learning Models with Pickle**  
   Learn about different ways to save machine learning models, including using the Pickle format.  
   [Saving ML Models with Pickle](https://www.alliage-ad.com/data-science/structures-de-sauvegarde-de-modele-de-machine-learning-le-format-pickle/)

---

